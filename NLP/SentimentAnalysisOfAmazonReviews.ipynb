{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Sentiment Analysis to Predict the Number of Stars in Amazon Reviews\n",
    "If you want to know how your customers feel about your products, you could read thousands of individual reviews or surveys. And how would you summarize what you have learned? Or you could train a sentiment analysis model to score the data and use the output to summarize how your customers feel.\n",
    "\n",
    "While sentiment analysis typically makes a binary prediction (positive vs. negative) using a classifier such as logistic regression. Amazon reviews provide a finer-grained perspective along a range of scores from 1 star (the most negative) to 5 stars (the most positive). This allows us to use a regression model to predict where along the range of sentiment a particular review falls.\n",
    "\n",
    "This experiment will also demonstrate how to use `sklearn.model_selection.GridSearchCV` to tune a preprocessing pipeline.\n",
    "\n",
    "Data set credit: *John Blitzer, Mark Dredze, Fernando Pereira. Biographies, Bollywood, Boom-boxes and Blenders: Domain Adaptation for Sentiment Classification. Association of Computational Linguistics (ACL), 2007.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from nltk import download, pos_tag\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "import string\n",
    "import re\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "from bs4 import BeautifulSoup\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Parameterization\n",
    "Just as GridSearchCV and RandomSearchCV can search for optimal model hyperparameters, they can search for the optimal preprocessing hyperparameters. Any data transformation class included in an optimizing pipeline must implement the following functions:\n",
    "+ `fit()`\n",
    "+ `transform()`\n",
    "+ `fit_transform()`\n",
    "+ `get_params()`\n",
    "+ `set_params()`\n",
    "\n",
    "By defining `sklearn.base.BaseEstimator` and `sklearn.base.TransformerMixin` as the base classes for a transformer, you get default implementations of `fit_transform()`, `get_params()`, and `set_params()` for free. Any transformers you plan to parameterize must override the `get_params()` and `set_params()` function; however, the default `fit_transform()` function should work for all custom transformers. Since most of these transformers only need to implement a `transform()` function, you can implement a TransformerBase class that defines the correct base classes and a default fit method, leaving the definition of a `transform()` method as the only remaining coding task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TransformerBase(BaseEstimator, TransformerMixin):\n",
    "    '''\n",
    "    Provides no-op fit() function for Transformers that only need\n",
    "    a fit method\n",
    "    '''\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "class LowerCaser(TransformerBase):\n",
    "    \n",
    "    def transform(self, X, **fit_params):\n",
    "        for i in range(len(X)):\n",
    "            X[i] = X[i].lower()\n",
    "        return X    \n",
    "\n",
    "class Tokenizer(TransformerBase):\n",
    "\n",
    "    def transform(self, X, **fit_params):\n",
    "        for i in range(len(X)):\n",
    "            X[i] = wordpunct_tokenize(X[i])\n",
    "            for tok in X[i]:\n",
    "                if tok.endswith('.') and len(tok) > 1:\n",
    "                    X[i].remove(tok)\n",
    "        return X\n",
    "    \n",
    "def remove_listed_tokens(X, removal_list):\n",
    "    '''\n",
    "    If you immediately remove a token as you iterate forward through a list,\n",
    "    you skip over the next token. This function instead builds a list of tokens\n",
    "    to be removed, then removes them at the end.\n",
    "    \n",
    "    Parameters\n",
    "      X - list of lists\n",
    "      removal_list - string of tokens (e.g., punctuation), or list of strings (e.g., stopwords)\n",
    "    '''\n",
    "    for doc in X:\n",
    "        removals = []\n",
    "        for tok in doc:\n",
    "            if tok in removal_list:\n",
    "                removals.append(tok)\n",
    "        for p in removals:\n",
    "            doc.remove(p)\n",
    "    return X\n",
    "        \n",
    "class StopWordRemover(TransformerBase):\n",
    "    \n",
    "    def __init__(self):\n",
    "        download(\"stopwords\")\n",
    "        \n",
    "    def transform(self, X, **fit_params):\n",
    "        return remove_listed_tokens(X, stopwords.words('english'))\n",
    "    \n",
    "class Stringizer(TransformerBase):\n",
    "    def transform(self, X, **fit_params):\n",
    "        for i in range(len(X)):\n",
    "            X[i] = ' '.join(X[i])\n",
    "        return X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have have tokenized data, you can part-of-speech (POS) tag them and *lemmatize* them. Lemmatizing substitutes a root word for a token; for example, *\"running\"* becomes *\"run\"*. It works best when you provide POS tags, but the Penn Treenet-based `nltk.pos_tag` function uses a different set of tags than the WordNetLemmatizer. Thus you must define a map between the two tag sets. Not every Treenet tag can be mapped; when a token's Treenet POS is not in the map keys, the token cannot be usefully lemmatized and is therefore skipped over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Lemmatizer(TransformerBase):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.treenet_map = defaultdict(str)\n",
    "        self.treenet_map['N'] = wordnet.NOUN\n",
    "        self.treenet_map['R'] = wordnet.ADV\n",
    "        self.treenet_map['V'] = wordnet.VERB\n",
    "        self.treenet_map['J'] = wordnet.ADJ\n",
    "        \n",
    "    def transform(self, X, **fit_params):\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        for i in range(len(X)):\n",
    "            doc = X[i].copy()\n",
    "            X[i] = [] # a list of lemmatized tokens\n",
    "            for tok, pos in pos_tag(doc):\n",
    "                wordnet_pos = self.treenet_map[pos[0]]\n",
    "                if not wordnet_pos:\n",
    "                    X[i].append(tok) # use tok without any lemmatizing if not a recognized POS\n",
    "                else:\n",
    "                    X[i].append(lemmatizer.lemmatize(tok, wordnet_pos))\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Parameterized Preprocessing Class\n",
    "In general, you would expect puntuation such as periods and quotation marks not to contribute much to sentiment analysis, so you would remove them from text data. However, it is *possible* that an exclamation mark or a question mark might indicate an emotion. How would you know? You run an experiment! The `exceptions` parameter is a string containing any punctuation marks that should be retained (i.e., excepted from the removal process). You must override the `get_params()` and `set_params()` functions so the appropriate search method (`sklearn.model_selection.GridSearchCV` or `sklearn.model_selection.RandomSearchCV`) can run experiments by setting the `exceptions` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PunctuationRemover(TransformerBase):\n",
    "    \n",
    "    def __init__(self, exceptions = ''):\n",
    "        self.exceptions = exceptions\n",
    "        \n",
    "    def transform(self, X, **fit_params):\n",
    "        if not self.exceptions:\n",
    "            punc = string.punctuation\n",
    "        else:\n",
    "            retained_punc = re.compile('['+self.exceptions+']') # don't remove these chars; they may convey emotion\n",
    "            punc = retained_punc.sub('', string.punctuation)\n",
    "        return remove_listed_tokens(X, punc)\n",
    "    \n",
    "    def get_params(self, deep=True):\n",
    "        return {'exceptions': self.exceptions}\n",
    "    \n",
    "    def set_params(self, **parameters):\n",
    "        for parm, value in parameters.items():\n",
    "            setattr(self, parm, value)\n",
    "        return self\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establishing a Baseline Model\n",
    "We first establish a baseline predictor, then we can optimize the proprocessing pipeline and, finally, perform model selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_pipeline = Pipeline([('lower', LowerCaser()),\n",
    "                          ('tokenize', Tokenizer()),\n",
    "                          ('lemmatize', Lemmatizer()),\n",
    "                          ('stopwords', StopWordRemover()),\n",
    "                          ('punc', PunctuationRemover()),\n",
    "                          ('stringize', Stringizer()),\n",
    "                          ('vec', CountVectorizer()),\n",
    "                          ('model', Lasso())])\n",
    "\n",
    "def get_reviews_and_ratings(path):\n",
    "    observations = BeautifulSoup(open(path).read())\n",
    "    reviews = observations.findAll('review_text')\n",
    "    reviews = [node.text for node in reviews]\n",
    "    ratings = observations.findAll('rating')\n",
    "    ratings = [float(node.text) for node in ratings]\n",
    "    return reviews, ratings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# load train and test data\n",
    "positive_path = '/data/electronics/positive.review'\n",
    "positive_reviews, positive_ratings = get_reviews_and_ratings(positive_path)\n",
    "negative_path = '/data/electronics/negative.review'\n",
    "negative_reviews, negative_ratings = get_reviews_and_ratings(negative_path)\n",
    "test_path = '/data/electronics/unlabeled.review'\n",
    "test_reviews, test_ratings = get_reviews_and_ratings(test_path)\n",
    "\n",
    "X = positive_reviews + negative_reviews\n",
    "Y = positive_ratings + negative_ratings\n",
    "shuffle_index = list(range(len(X)))\n",
    "random.shuffle(shuffle_index)\n",
    "X = list(map(X.__getitem__, shuffle_index))\n",
    "Y = list(map(Y.__getitem__, shuffle_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using our baseline pipeline, let's establish a baseline model. Let's tune a linear regression model with Lasso (*L1*) regularization so we can get an informative model that's easy to compute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'model__alpha': [0.03, 0.07, 0.1, 0.2, 0.5, 1.0]}\n",
    "gs = GridSearchCV(base_pipeline, param_grid, cv = 5)\n",
    "gs.fit(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what the model tells us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized hyperparameters:====================\n",
      "model__alpha: 0.03\n",
      "\n",
      "Predictive words and coefficients====================\n",
      "return             -0.68033278428\n",
      "bad                -0.403256279195\n",
      "waste              -0.403132601845\n",
      "try                -0.228285563944\n",
      "send               -0.172963391161\n",
      "work               -0.170962957798\n",
      "back               -0.162752200524\n",
      "poor               -0.152085747623\n",
      "item               -0.13231479252\n",
      "even               -0.102432704422\n",
      "customer           -0.0989887349194\n",
      "support            -0.094425899632\n",
      "software           -0.0883674461114\n",
      "get                -0.0852166447041\n",
      "buy                -0.0694080124812\n",
      "would              -0.0527671633476\n",
      "month              -0.0515337508775\n",
      "warranty           -0.049498778835\n",
      "tell               -0.0404522328405\n",
      "money              -0.0315761809083\n",
      "card               -0.0297028487524\n",
      "company            -0.0291649560937\n",
      "unit               -0.0277561885853\n",
      "product            -0.0232889177456\n",
      "another            -0.022519067416\n",
      "ipod               -0.0155416315244\n",
      "keyboard           -0.00496139226585\n",
      "purchase           -0.00301705814072\n",
      "dvd                3.36607092044e-06\n",
      "screen             0.00516920705963\n",
      "usb                0.00760291702722\n",
      "laptop             0.00762140139703\n",
      "listen             0.0107121508441\n",
      "mouse              0.0107926820805\n",
      "memory             0.0110054377544\n",
      "need               0.0176597777744\n",
      "lot                0.0225740942578\n",
      "look               0.03036699808\n",
      "speaker            0.0396774944684\n",
      "also               0.0407456293573\n",
      "like               0.0420918666574\n",
      "quality            0.0425972391427\n",
      "small              0.0430241794996\n",
      "little             0.0796808409414\n",
      "happy              0.0901401789318\n",
      "good               0.0913255909865\n",
      "love               0.115260195209\n",
      "use                0.14493717293\n",
      "best               0.149366061196\n",
      "well               0.149796510378\n",
      "perfect            0.192540350365\n",
      "easy               0.282700532024\n",
      "price              0.321441055098\n",
      "excellent          0.332493163955\n",
      "great              0.336320124108\n",
      "highly             0.362050510071\n",
      "\n",
      "Y intercept: 3.04241277505\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "best_estimator = gs.best_estimator_ # a Pipeline object\n",
    "model = best_estimator.named_steps['model'] # a Lasso model\n",
    "vec = best_estimator.named_steps['vec'] # a CountVectorizer\n",
    "\n",
    "# Which model hyperparameter got the best cross-validation results?\n",
    "print(\"Optimized hyperparameters:\" + '=' * 20)\n",
    "best_parameters = best_estimator.get_params()\n",
    "for param_name in sorted(param_grid.keys()):\n",
    "    print(\"%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "print()\n",
    "\n",
    "# Linear model coefficients\n",
    "feature_map = {} # index -> word\n",
    "for k in vec.vocabulary_: \n",
    "    feature_map[vec.vocabulary_[k]] = k\n",
    "# significant words have non-zero coefficients\n",
    "significant_word_index = np.where(model.coef_ != 0.)[0]\n",
    "words_and_coefs = []\n",
    "for i in significant_word_index[:]:\n",
    "    words_and_coefs.append((feature_map[i], model.coef_[i]))\n",
    "sorted_words = sorted(words_and_coefs, key = lambda tup: tup[1])\n",
    "print(\"Predictive words and coefficients\" + '=' * 20)\n",
    "for tup in sorted_words:\n",
    "    print('{:<18}'.format(tup[0]), tup[1])\n",
    "print()\n",
    "print(\"Y intercept:\", model.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean absolute error: 1.329\n"
     ]
    }
   ],
   "source": [
    "# Model accuracy on test data\n",
    "predictions = gs.predict(test_reviews)\n",
    "predictions[predictions < 1] = 1.0\n",
    "predictions[predictions > 5.0] = 5.0\n",
    "print(\"mean absolute error of model: %0.3f\" % metrics.mean_absolute_error(test_ratings, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Let's compare the baseline model with predicting the mean of the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean absolute error of predicting the training mean: 1.679\n"
     ]
    }
   ],
   "source": [
    "mean_prediction = np.mean(Y)\n",
    "mean_predictions = np.array([mean_prediction for _ in range(len(test_ratings))])\n",
    "print(\"mean absolute error of predicting the training mean: %0.3f\" % metrics.mean_absolute_error(test_ratings, mean_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The baseline model does not seem like a Kaggle competition winner, but it *is* significantly better than a mildly informed baseline prediction. \n",
    "\n",
    "### Important Features\n",
    "According to the model, the most informative **negative** words in Amazon electronics reviews are as follows:\n",
    "\n",
    "| Lemma | Coefficient |\n",
    "| --- | --- |\n",
    "| return | -0.68033278428 |\n",
    "| bad  | -0.403256279195 |\n",
    "| waste | -0.403132601845 |\n",
    "| try | -0.228285563944 |\n",
    "| send | -0.172963391161 |\n",
    "| work | -0.170962957798 |\n",
    "| back | -0.162752200524 |\n",
    "| poor | -0.152085747623 |\n",
    "\n",
    "And the most informative **positive** words are:\n",
    "\n",
    "| Lemma | Coefficient |\n",
    "| --- | --- |\n",
    "| highly | 0.362050510071 |\n",
    "| great | 0.336320124108 |\n",
    "| excellent | 0.332493163955 |\n",
    "| price | 0.321441055098 |\n",
    "| easy |  0.282700532024 |\n",
    "| perfect | 0.192540350365 |\n",
    "| well  | 0.149796510378 |\n",
    "| best |  0.149366061196 |\n",
    "\n",
    "Most of these are intuitive: adjectives/adverbs like *bad, back,* and *poor* would surely express disappointment, and *highly, great, excellent, easy, perfect, well,* and *best* would express satisfaction. If you are talking about *try*ing your purchase,  it probably didn't work--so that would be negative. Ditto for *return, waste* (who wants to waste money?), and *send* (presumably because you had to return it). *Price* is a curious positive word, as not every price is good. It seems happy reviewers are more likely to talk about *price* and dissatisfied reviewers are more likely to talk about *returns*.\n",
    "\n",
    "Worth noting if you are a product manager: *ipod* has negative connotations, but *speakers* seem to be popular."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizing Engineered Features\n",
    "The baseline pipeline removed all punctuation...but did that remove useful information? Would a question mark, an exclamation mark, or a dollar sign indicate sentiment? Let's see if including some combination of them helps to improve the predictive model.\n",
    "\n",
    "*Note: if you are re-running this notebook, you must re-load the data. The first data pipeline already munged the data, so you must start over with freshly loaded data.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that we are fixing the alpha parameter of the Lasso model\n",
    "pipeline = Pipeline([('lower', LowerCaser()),\n",
    "                     ('tokenize', Tokenizer()),\n",
    "                     ('lemmatize', Lemmatizer()),\n",
    "                     ('stopwords', StopWordRemover()),\n",
    "                     ('punc', PunctuationRemover()),\n",
    "                     ('stringize', Stringizer()),\n",
    "                     ('vec', CountVectorizer()),\n",
    "                     ('model', Lasso(alpha = 0.03))])\n",
    "param_grid = {'punc__exceptions': ['', '?', '$', '!', '?$', '?!', '$!', '?$!']}\n",
    "gs = GridSearchCV(pipeline, param_grid, cv = 5)\n",
    "gs.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized hyperparameters:====================\n",
      "punc__exceptions: ''\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Which model hyperparameter got the best cross-validation results?\n",
    "best_estimator = gs.best_estimator_ # a Pipeline object\n",
    "print(\"Optimized hyperparameters:\" + '=' * 20)\n",
    "best_parameters = best_estimator.get_params()\n",
    "for param_name in sorted(param_grid.keys()):\n",
    "    print(\"%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Punctuation Hypothesis Was Wrong!\n",
    "The data show that adding the most salient punctuation marks to the bag of words does not help to predict Amazon ratings. That conclusion surprises me, but when you work as a data scientist every day harbors a new surprise.\n",
    "\n",
    "Now that the best configuration of the data pipeline is known, we could go through the process of model selection and tuning in order to reduce the prediction error. However, that is a topic for another Python notebook. I hope you have enjoyed learning along with me!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
