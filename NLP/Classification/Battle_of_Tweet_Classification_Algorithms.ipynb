{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Battle of the Tweet Classification Algorithms!\n",
    "Having written [an implementation of the Naive Bayes algorithm](https://github.com/chrisfalter/DataScience/tree/master/NLP/NaiveBayes) to predict the geolocation of test tweets from a tweet corpus, I thought it would be worthwhile to see how it compares to other algorithms that can be used for document classification. The scikit-learn library provides a terrific toolkit for this exploration because:\n",
    "+ It has a plethora of classification algorithms\n",
    "+ The `sklearn.model_selection.GridSearchCV` class provides an easy-to-use API to optimize a classifier's hyperparameters\n",
    "\n",
    "This experiment compares the accuracy of the following algorithms:\n",
    "+ Naive Bayes\n",
    "+ Adaboost\n",
    "+ Random Forest\n",
    "+ K-Nearest Neighbors\n",
    "+ Gradient Boost\n",
    "+ Model Stack of Best 3 Models\n",
    "\n",
    "You will also learn how to use `sklearn.pipeline.Pipeline` and `sklearn.preprocessing.FunctionTransformer` to create a custom data preprocessing pipeline.\n",
    "\n",
    "## The Limits of This Experiment\n",
    "Machine learning's \"No Free Lunch\" theorem states that no single algorithm gives the best predictions for all problems; the ability to solve one class of problems well is gained at the expense of doing more poorly on some other class of problems. Therefore do not use the outcome of this experiment to justify using the tweet classification winner as your algorithm of choice to answer all data questions! Instead, be informed and inspired by this experiment to conduct your own experiment to find which classifier (with which hyperparameters) will best answer your questions.\n",
    "\n",
    "Let's begin by importing some classes and namespaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import download\n",
    "import string\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Data Pipeline from Transform Functions\n",
    "Scikit-learn provides an API to build a pipeline of transformers to munge, cleanse, transform, and feature-engineer your data set. I wrote a variety of functions that conform to the expected interface for a data transformer. It is a simple matter to use the `sklearn.preprocessor.FunctionTransformer` class to turn them into building blocks of a pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\582139\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Assumes a list of strings, one string per doc. Use before tokenization of docs.\n",
    "def lowercase(X):\n",
    "    for i in range(len(X)):\n",
    "        X[i] = X[i].lower()\n",
    "    return X    \n",
    "    \n",
    "specialEntitiesRegex = re.compile(\"&[a-z]+;\")\n",
    "\n",
    "# Assumes a list of strings, one string per doc. Use before tokenization of docs.\n",
    "def removeSpecialEntities(X):\n",
    "    for i in range(len(X)):\n",
    "        s = X[i]\n",
    "        m = specialEntitiesRegex.search(s)\n",
    "        while (m):\n",
    "            s = s[:m.start()] + s[m.end():]\n",
    "            m = specialEntitiesRegex.search(s)\n",
    "        X[i] = s\n",
    "    return X\n",
    "\n",
    "printable = set(string.printable)\n",
    "\n",
    "# Assumes a list of strings, one string per doc. Use before tokenization of docs.\n",
    "def removeUnprintable(X):\n",
    "    for i in range(len(X)):\n",
    "        s = ''.join(filter(lambda x: x in printable, X[i]))\n",
    "        X[i] = s\n",
    "    return X\n",
    "    \n",
    "punc = string.punctuation.replace(' ','') # don't remove spaces! They demarcate words\n",
    "punctuationRemover = str.maketrans(punc, ' '*len(punc))\n",
    "\n",
    "# Assumes a list of strings, one string per doc. Use before tokenization of docs.\n",
    "def removePunctuation(X):\n",
    "    for i in range(len(X)):\n",
    "        s = X[i].translate(punctuationRemover)\n",
    "        X[i] = s\n",
    "    return X\n",
    "\n",
    "def tokenize(X):\n",
    "    for i in range(len(X)):\n",
    "        X[i] = X[i].split()\n",
    "    return X\n",
    "    \n",
    "cityInitials = ['la', 'orl', 'washing']\n",
    "\n",
    "# Assumes a list of lists, one list of tokens per doc. Use after tokenization of docs.\n",
    "def generateCityInitialsTokens(X):\n",
    "    for i in range(len(X)):\n",
    "        doc = X[i]\n",
    "        initialList = []\n",
    "        for token in doc:\n",
    "            for initials in cityInitials:\n",
    "                if token.startswith(initials):\n",
    "                    initialList.append(initials)\n",
    "        if (initialList):\n",
    "            doc += initialList\n",
    "            X[i] = doc\n",
    "    return X\n",
    "\n",
    "monikers = set(['ny','york','manh',\n",
    "            'dc',\n",
    "            'bost',\n",
    "            'chic','illin',\n",
    "            'hollw','angel',\n",
    "            'orlan','fl',\n",
    "            'sf','bay','fran',\n",
    "            'tl','georgia',\n",
    "            'houst','tx','tex',\n",
    "            'phil','delph','penn',\n",
    "            'sd','dieg',\n",
    "            'toro','canad','ontar'])\n",
    "\n",
    "# Assumes a list of lists, one list of tokens per doc. Use after tokenization of docs.\n",
    "def generateMonikerTokens(X):\n",
    "    for i in range(len(X)):\n",
    "        doc = X[i]\n",
    "        monikerList = []\n",
    "        for tok in doc:\n",
    "            for moniker in monikers:\n",
    "                if moniker in tok:\n",
    "                    monikerList.append(moniker)\n",
    "        if monikerList:\n",
    "            doc += monikerList\n",
    "            X[i] = doc\n",
    "    return X\n",
    "\n",
    "# make sure the list of stop words is available\n",
    "download(\"stopwords\")\n",
    "commonTweetWords = ['job','hiring','jobs','careerarc','street','opening','work','apply','st'] \n",
    "commonTweetWords += list(\"abcdefghijklmnopqrstuvwxyz\") + list(\"01234456789\")\n",
    "stopWords = set(stopwords.words('english') + commonTweetWords)\n",
    "\n",
    "# Assumes a list of lists, one list of tokens per doc. Use after tokenization of docs.\n",
    "def removeStopWords(X):\n",
    "    '''\n",
    "    Assumes a list of lists, one list of tokens per doc. Use after tokenization of docs.\n",
    "    '''\n",
    "    for i in range(len(X)):\n",
    "        doc = X[i]\n",
    "        doc = list(filter(lambda t: t not in stopWords, doc))\n",
    "        X[i] = doc\n",
    "    return X\n",
    "\n",
    "# Use at end of preprocessing pipeline to convert doc-as-tokens into a string, so it can be used by a vectorizer\n",
    "def stringize(X):\n",
    "    for i in range(len(X)):\n",
    "        X[i] = ' '.join(X[i])\n",
    "    return X\n",
    "\n",
    "preprocessor = Pipeline(steps=[\n",
    "    ('Lower Case', FunctionTransformer(lowercase, validate=False)), \n",
    "    ('SpecialEntities', FunctionTransformer(func = removeSpecialEntities, validate = False)),\n",
    "    (\"Unprintable\",FunctionTransformer(func = removeUnprintable, validate = False)),\n",
    "    (\"Punctuation\",FunctionTransformer(removePunctuation, validate = False)),\n",
    "    (\"Tokenize\",FunctionTransformer(tokenize, validate=False)),\n",
    "    (\"City Initials\", FunctionTransformer(generateCityInitialsTokens, validate=False)), \n",
    "    (\"Monikers\", FunctionTransformer(generateMonikerTokens, validate=False)), \n",
    "    (\"Stop Words\", FunctionTransformer(removeStopWords, validate=False)), \n",
    "    ('Stringize', FunctionTransformer(stringize, validate = False))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's load our training and test data into the preprocessing pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLocationAndTweet(s):\n",
    "    '''\n",
    "    Returns a tuple (location, tweet)\n",
    "    location = first token in string s\n",
    "    tweet = remainder of string s\n",
    "    '''\n",
    "    divider = s.find(' ')\n",
    "    return s[:divider], s[divider + 1:]\n",
    "\n",
    "def getXandY(rawData):\n",
    "    X, Y = [],[]\n",
    "    for line in rawData:\n",
    "        location, tweet = getLocationAndTweet(line)\n",
    "        X.append(tweet)\n",
    "        Y.append(location)\n",
    "    return X, Y\n",
    "        \n",
    "train_path = 'C:/NLP/ClassificationSklearn/tweets.train.txt'\n",
    "test_path = 'C:/NLP/ClassificationSklearn/tweets.test1.txt'\n",
    "with open(train_path, 'r', encoding='latin1', newline='\\n') as trainTweets:\n",
    "    X_train, Y_train = getXandY(trainTweets)\n",
    "with open(test_path, 'r', encoding='latin1', newline='\\n') as testTweets:\n",
    "    X_test, Y_test = getXandY(testTweets)    \n",
    "X_train = preprocessor.transform(X_train)\n",
    "X_test = preprocessor.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's transform the preprocessed tweets into vectorized (sparse array) representations that the scikit-learn classifiers can use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(lowercase = False)\n",
    "X_train_cv = count_vectorizer.fit_transform(X_train, Y_train)\n",
    "X_test_cv = count_vectorizer.transform(X_test)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_cv)\n",
    "X_test_tfidf = tfidf_transformer.fit_transform(X_test_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time to Test\n",
    "\n",
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes w/count:   0.698\n",
      "Naive Bayes w/tf-idf:   0.614\n"
     ]
    }
   ],
   "source": [
    "def fit_and_score(classifier, X_tr, Y_tr, X_te, Y_te):\n",
    "    classifier.fit(X_tr, Y_tr)\n",
    "    predictions = classifier.predict(X_te)\n",
    "    return metrics.accuracy_score(Y_te, predictions)\n",
    "\n",
    "print(\"Naive Bayes w/count:   %0.3f\" % fit_and_score(MultinomialNB(), X_train_cv, Y_train, X_test_cv, Y_test))\n",
    "print(\"Naive Bayes w/tf-idf:   %0.3f\" % fit_and_score(MultinomialNB(), X_train_tfidf, Y_train, X_test_tfidf, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MultinomialNB classifier with straightforward counts obtains the identical result to [the Naive Bayes classifier that I manually coded](https://github.com/chrisfalter/DataScience/tree/master/NLP/NaiveBayes)! My intuition that TF-IDF would degrade model accuracy seems accurate, as well.\n",
    "\n",
    "### Tune the Hyperparameters\n",
    "Now let's use sklearn's ability to search over hyperparameters to tune our Naive Bayes classifier. Given the small number of combinations that this code will examine, we can use the exhaustive search strategy provided by [the GridSearchCV method](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html). If we had a large state space of hyperparameters to explore, we might first use [RandomizedSearchCV](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html) to narrow our search to the most promising region of hyperparameter settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tune_classifier(classifier, clf_name, parameter_space, X_tr, Y_tr, X_te, Y_te):\n",
    "    estimator = Pipeline([(clf_name, classifier)])\n",
    "    grid_search = GridSearchCV(estimator, parameter_space, n_jobs=3, cv=5) # 5-fold cross-validation\n",
    "    grid_search.fit(X_tr, Y_tr)\n",
    "    best_estimator = grid_search.best_estimator_\n",
    "    best_parameters = best_estimator.get_params()\n",
    "    for param_name in sorted(parameter_space.keys()):\n",
    "        print(\"%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    predictions = best_estimator.predict(X_te)\n",
    "    print(clf_name + \" tuned model accuracy: %0.3f\" % metrics.accuracy_score(Y_te, predictions))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB__alpha: 0.40000000000000002\n",
      "NB tuned model accuracy: 0.694\n"
     ]
    }
   ],
   "source": [
    "param_name = \"NB__alpha\"\n",
    "parameter_space = {param_name: np.array(range(1, 11))/10}\n",
    "tune_classifier(MultinomialNB(), \"NB\",  parameter_space, X_train_cv, Y_train, X_test_cv, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Curiously, after hyperparameter tuning the model performs slightly worse on the test set. Do not conclude, however, that you should not tune hyperparameters! In all likelihood, the tuned model should generalize better to a typical, large test set. There might be something just a little bit atypical about our test set.\n",
    "\n",
    "Now let's try some other algorithms.\n",
    "### K-Nearest Neighbors, Random Forest, and Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KN__n_neighbors: 11\n",
      "KN__p: 2\n",
      "KN tuned model accuracy: 0.464\n",
      "RF__max_features: 'log2'\n",
      "RF__n_estimators: 250\n",
      "RF tuned model accuracy: 0.706\n",
      "Ada__learning_rate: 1.0\n",
      "Ada__n_estimators: 150\n",
      "Ada tuned model accuracy: 0.600\n",
      "Ada__learning_rate: 0.6\n",
      "Ada__n_estimators: 150\n",
      "Ada tuned model accuracy: 0.602\n"
     ]
    }
   ],
   "source": [
    "# Because of the high number of classes (12), we specify a high number of neighbors in K-neighbors classifier.\n",
    "# p = 2 so Minkowski = Euclidean distance. Because the TF-IDF vector is L2-normalized, this yields same neighbor ranking\n",
    "# as cosine similarity.\n",
    "kn_parm_space = {\"KN__n_neighbors\": [11, 13, 15], \n",
    "                 \"KN__p\": [2]} \n",
    "tune_classifier(KNeighborsClassifier(), \"KN\", kn_parm_space, X_train_tfidf, Y_train, X_test_tfidf, Y_test)\n",
    "\n",
    "rf_parm_space = {\"RF__n_estimators\": [10, 50, 100, 250],\n",
    "                 \"RF__max_features\": [\"log2\"]}\n",
    "tune_classifier(RandomForestClassifier(), \"RF\", rf_parm_space, X_train_cv, Y_train, X_test_cv, Y_test)\n",
    "\n",
    "ada_parm_space = {\"Ada__n_estimators\": [50, 100, 150], \n",
    "                  \"Ada__learning_rate\": [0.2, 0.6, 1.0]}\n",
    "tune_classifier(AdaBoostClassifier(), \"Ada\",  ada_parm_space, X_train_cv, Y_train, X_test_cv, Y_test)\n",
    "tune_classifier(AdaBoostClassifier(), \"Ada\",  ada_parm_space, X_train_tfidf, Y_train, X_test_tfidf, Y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-Nearest Neighbors and Adaboost don't seem to compete effectively in predicting tweet geological origins. However, Random Forest is doing well. Since its best result came at the maximum value of the hyperparameter `n_estimators`, let's extend the range of `n_estimators` values further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF__max_features: 'log2'\n",
      "RF__n_estimators: 800\n",
      "RF tuned model accuracy: 0.708\n"
     ]
    }
   ],
   "source": [
    "rf_parm_space = {\"RF__n_estimators\": [200, 400, 600, 800],\n",
    "                 \"RF__max_features\": [\"log2\"]}\n",
    "tune_classifier(RandomForestClassifier(), \"RF\", rf_parm_space, X_train_cv, Y_train, X_test_cv, Y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No dice! Adding model complexity by tripling the number of estimators had almost no effect on accuracy. So it is probably better to stick with 250 decision trees in the random forest, given that each tree would have a maximum of log2 features.\n",
    "\n",
    "My off-line testing shows that the training vectors `X_train_cv` and `X_train_tfidf` have 32,000 features (i.e., words). This means that each decision tree in the random forest would use a maximum of 14 features (log2(32000)). Let's set the maximum tree features to the square root of 32000 (178) to see if that improves classification accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF__max_features: 'sqrt'\n",
      "RF__n_estimators: 90\n",
      "RF tuned model accuracy: 0.684\n"
     ]
    }
   ],
   "source": [
    "rf_parm_space = {\"RF__n_estimators\": [15, 40, 90, 150],\n",
    "                 \"RF__max_features\": [\"sqrt\"]}\n",
    "tune_classifier(RandomForestClassifier(), \"RF\", rf_parm_space, X_train_cv, Y_train, X_test_cv, Y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting\n",
    "Like the Random Forest algorithm, Gradient Boosting builds an ensemble of decision trees. However, the trees are generated to minimize a loss function along a gradient, rather than randomly as in the random forest method. Let's see how well it classifies tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GB__max_depth: 5\n",
      "GB__n_estimators: 300\n",
      "GB tuned model accuracy: 0.672\n"
     ]
    }
   ],
   "source": [
    "gb_parm_space = {\"GB__n_estimators\": [80, 150, 300],\n",
    "                 \"GB__max_depth\": [3,4,5]}\n",
    "tune_classifier(GradientBoostingClassifier(), \"GB\", gb_parm_space, X_train_cv, Y_train, X_test_cv, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Stacking\n",
    "Predicting with a stack of models is the machine learning equivalent of \"the wisdom of crowds\"; the strengths of the various models compensate for the weaknesses of any one model. The procedure is straightforward: Select the 3 most desirable models, configure them according to the experiments conducted so far, then combine their votes into a single prediction. Sophisticated practitioners with plenty of time on their hands could stack many more than 3 models, but the Random Forest, Naive Bayes, and Gradient Boosting classifiers will suffice for our purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stack w/uniform soft voting:   0.704\n",
      "Stack w/weighted hard voting:   0.706\n"
     ]
    }
   ],
   "source": [
    "rf_clf = RandomForestClassifier(n_estimators = 250, max_features = 'log2')\n",
    "nb_clf = MultinomialNB(alpha = 0.4)\n",
    "gb_clf = GradientBoostingClassifier(max_depth = 5, n_estimators = 300)\n",
    "\n",
    "soft_stack = VotingClassifier([('rf', rf_clf), ('nb', nb_clf), ('gb', gb_clf)], voting = 'soft', n_jobs = 1)\n",
    "print(\"Stack w/uniform soft voting:   %0.3f\" % fit_and_score(soft_stack, X_train_cv, Y_train, X_test_cv, Y_test))\n",
    "\n",
    "hard_stack = VotingClassifier([('rf', rf_clf), ('nb', nb_clf), ('gb', gb_clf)], voting = 'hard', weights = [3, 2, 2], n_jobs = 1)\n",
    "print(\"Stack w/weighted hard voting:   %0.3f\" % fit_and_score(hard_stack, X_train_cv, Y_train, X_test_cv, Y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Winner Is ... It Depends\n",
    "Random forest gets the top accuracy score of 70.8%, followed closely by the model stack at 70.6% and Naive Bayes at 69.4%. However, outside of Kaggle contests, accuracy is not the only criterion in selecting an algorithm. For example, let's see how long the algorithms run to obtain their results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Training Race ==========\n",
      "Naive Bayes:   0.1235\n",
      "Random Forest: 842.6814\n",
      "\n",
      "========== Prediction Race ==========\n",
      "Naive Bayes:   0.0035\n",
      "Random Forest: 0.6795\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# time measurement for training\n",
    "nb_start = time.time()\n",
    "nb_clf = MultinomialNB(alpha = 0.4)\n",
    "nb_clf.fit(X_train_cv, Y_train)\n",
    "nb_training_time = time.time() - nb_start\n",
    "\n",
    "rf_start = time.time()\n",
    "rf_clf = RandomForestClassifier(n_estimators = 250, max_features = 'log2')\n",
    "rf_clf.fit(X_train_cv, Y_train)\n",
    "rf_training_time = time.time() - rf_start\n",
    "\n",
    "# time measurement for prediction\n",
    "nb_start = time.time()\n",
    "nb_clf.predict(X_test_cv)\n",
    "nb_predict_time = time.time() - nb_start\n",
    "\n",
    "rf_start = time.time()\n",
    "rf_clf.predict(X_test_cv)\n",
    "rf_predict_time = time.time() - rf_start\n",
    "\n",
    "# display results\n",
    "print('=' * 10, 'Training Race', '=' * 10)\n",
    "print(\"Naive Bayes:   %0.4f\" % nb_training_time)\n",
    "print(\"Random Forest: %0.4f\" % rf_training_time)\n",
    "print()\n",
    "print('=' * 10, 'Prediction Race', '=' * 10)\n",
    "print(\"Naive Bayes:   %0.4f\" % nb_predict_time)\n",
    "print(\"Random Forest: %0.4f\" % rf_predict_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes trains *almost 4 orders of magnitude faster than random forest, and predicts 200 times faster*. If you are renting your compute from AWS, Azure, or Google, using Naive Bayes instead of Random Forest could provide huge cost savings.\n",
    "\n",
    "And now let's see how much we can learn about how various features contribute to the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['chic', 'ny', 'york', 'chicago', 'houston', 'houst', 'bost', 'angel', 'tx', 'fran', 'new', 'il', 'ca', 'atlanta', 'orl', 'toronto', 'dieg', 'toro', 'washing', 'phil', 'boston', 'dc', 'tl', 'washington', 'nyc', 'los', 'san', 'sanfrancisco', 'delph', 'angeles', 'la', 'losangeles', 'ga', 'orlan', 'francisco', 'sandiego', 'philadelphia', 'orlando', 'orlpol', 'newyork']\n"
     ]
    }
   ],
   "source": [
    "def topFeaturesInRF(rf_clf, vec, n):\n",
    "    top_features = []\n",
    "    feature_map = {} # key = index, value = term\n",
    "    for k in vec.vocabulary_:\n",
    "        feature_map[vec.vocabulary_[k]] = k\n",
    "    top_index =  np.argsort(-rf_clf.feature_importances_)[:n]\n",
    "    for i in range(len(top_index)):\n",
    "        top_features.append(feature_map[top_index[i]])\n",
    "    return top_features\n",
    "\n",
    "top_rf_features = topFeaturesInRF(rf_clf, count_vectorizer, 40)\n",
    "print(top_rf_features)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Atlanta,_GA - ['tl', 'atlanta', 'ga', 'georgia', 'la', 'ny', 'atl', 'latest', 'click', 'fl']\n",
      "Boston,_MA - ['bost', 'boston', 'la', 'latest', 'report', 'massachusetts', 'see', 'click', 'ny', 'great']\n",
      "Chicago,_IL - ['chic', 'chicago', 'il', 'la', 'illin', 'ny', 'illinois', 'tl', 'latest', 'click']\n",
      "Houston,_TX - ['tx', 'houst', 'houston', 'la', 'latest', 'tex', 'click', 'texas', 'nursing', 'healthcare']\n",
      "Los_Angeles,_CA - ['la', 'angel', 'ca', 'los', 'angeles', 'losangeles', 'hollywood', 'tl', 'california', 'ny']\n",
      "Manhattan,_NY - ['ny', 'york', 'new', 'nyc', 'la', 'newyork', 'manh', 'fl', 'manhattan', 'park']\n",
      "Orlando,_FL - ['orl', 'fl', 'orlan', 'orlpol', 'orlando', 'opd', 'ave', 'la', 'dr', 'rd']\n",
      "Philadelphia,_PA - ['phil', 'delph', 'philadelphia', 'pa', 'la', 'philly', 'penn', 'ny', 'pennsylvania', 'tl']\n",
      "San_Diego,_CA - ['dieg', 'ca', 'san', 'sandiego', 'diego', 'la', 'sd', 'ny', 'california', 'latest']\n",
      "San_Francisco,_CA - ['fran', 'ca', 'san', 'sanfrancisco', 'francisco', 'sf', 'la', 'california', 'request', 'ny']\n",
      "Toronto,_Ontario - ['toro', 'toronto', 'la', 'york', 'canad', 'ontar', 'ontario', 'trucks', 'canada', 'ny']\n",
      "Washington,_DC - ['dc', 'washing', 'washington', 'la', 'ny', 'latest', 'day', 'district', 'great', 'see']\n"
     ]
    }
   ],
   "source": [
    "def topFeaturesInNB(nb_clf, vec, n):\n",
    "    '''\n",
    "    n = number of features per class\n",
    "    '''\n",
    "    top_features = {}\n",
    "    class_names = nb_clf.classes_\n",
    "    feature_map = {} # key = index, value = term\n",
    "    for k in vec.vocabulary_:\n",
    "        feature_map[vec.vocabulary_[k]] = k\n",
    "    for i in range(nb_clf.feature_log_prob_.shape[0]):\n",
    "        index_top = np.argsort(-nb_clf.feature_log_prob_[i,:])[:n]\n",
    "        features_for_class = []\n",
    "        for idx in index_top:\n",
    "            features_for_class.append(feature_map[idx])\n",
    "        top_features[class_names[i]] = features_for_class\n",
    "    return top_features \n",
    "\n",
    "nb_clf = MultinomialNB(alpha = 0.4)\n",
    "nb_clf.fit(X_train_cv, Y_train)\n",
    "top_features = topFeaturesInNB(nb_clf, count_vectorizer, 10)\n",
    "for city in top_features:\n",
    "    print(city, \"-\", top_features[city])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that Naive Bayes provides *much better feature information than Random Forest*. For example, the second most important feature according to the Random Forest `feature_importances_` list is `'ny'`. You would naturally assume that this would very strongly predict a geolocation of Manhattan; but you might be wrong. The Naive Bayes `feature_log_prob_` list shows that `'ny'` also appears among the top 10 predictors for 9 other cities. Perhaps the feature generation of the `ny` token is not working as expected due to words like \"irony\" or \"phony\" in tweets. The detailed information provided by Naive Bayes indicates that removing `'ny'` from the feature-generation would be worth trying.\n",
    "\n",
    "### Conclusion: Rules of Thumb\n",
    "For the tweet geolocation classification problem, here are the heuristics you could use in model selection:\n",
    "+ In exploratory stages, until your preprocessing pipeline is stable: use Naive Bayes due to its dramatic advantages in speed and cost.\n",
    "+ In production:\n",
    "  + If you need to explain the details of your algorithm to regulators or customers: use Naive Bayes\n",
    "  + If the higher compute costs of random forest outweigh its slightly better accuracy: use Naive Bayes\n",
    "  + If accuracy trumps everything: use Random Forest\n",
    "+ In a Kaggle competition: use Random Forest\n",
    "\n",
    "This experiment did not examine the capabilities of a recurrent neural network (RNN). An RNN would be even more complex than a random forest and yield even less information about feature importance, but its (presumably) superior accuracy might justify its use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
