# Natural Language Processing
Much of the approximately 30 zettabytes of data in the world exists in the form of unstructured or semi-structured documents, which may be in text or audio format (web pages, emails, voice recordings, Facebook posts, podcasts, tweets, and so forth). Natural language processing (NLP) is the interaction of machine learning with these data. Traditional topics in NLP include document search, automated translation, voice recognition, text classification, and text-to-speech; in 2018, the cutting edge of NLP includes chatbots and a Google assistant [that can schedule a haircut or make a restaurant reservation](https://www.youtube.com/watch?v=VZ9MBYfu_0A). 

This section of my repo contains my explorations and insights in NLP.

### Topics:
+ [Document Classification with Naive Bayes](https://github.com/chrisfalter/DataScience/tree/master/NLP/NaiveBayes) - One of the main NLP tasks is document classification. Look through this project if you want to understand how the Naive Bayes algorithm works and how it can be applied to the geolocation of tweets.
+ [Spam Classification with PySpark](https://github.com/chrisfalter/DataScience/blob/master/ML_Spark/SpamClassifier_SPARK.ipynb) - This notebook uses logistic regression to build a spam classifier against the CSMDC 2010 Spam data set, which has 4327 labeled observations. It uses the `PySpark` interface to Spark MLLib, which provides a lot of "big data" functionality. However, the exploration runs into some serious shortcomings in `pyspark.mllib` for real-world use. Fortunately, Spark also provides updated machine learning functionality in the `pyspark.ml` library, which future notebooks will explore.
+ [Battle of the Tweet Classification Algorithms](https://github.com/chrisfalter/DataScience/blob/master/NLP/Battle_of_Tweet_Classification_Algorithms.ipynb) - Having written an implementation of the Naive Bayes algorithm to predict the geolocation of test tweets from a tweet corpus, I thought it would be worthwhile to see how it compares to other algorithms that can be used for document classification. This experiment uses scikit-learn's model selection features to compare the following algorithms:
  + Naive Bayes
  + Adaboost
  + Random Forest
  + K-Nearest Neighbors
  + Gradient Boost
  + Model Stack of Best 3 Models
+ [Using Sentiment Analysis to Predict the Number of Stars in Amazon Reviews](https://github.com/chrisfalter/DataScience/blob/master/NLP/SentimentAnalysisOfAmazonReviews.ipynb) - If you want to know how your customers feel about your products, you could read thousands of individual reviews or surveys. And how would you summarize what you have learned? Or you could train a sentiment analysis model to score the data and use the output to summarize how your customers feel. While sentiment analysis typically makes a binary prediction (positive vs. negative) using a classifier such as logistic regression, Amazon reviews provide a finer-grained perspective along a range of scores from 1 star (the most negative) to 5 stars (the most positive). This allows us to use a regression model to predict where along the range of sentiment a particular review falls. This experiment will also demonstrate how to use `sklearn.model_selection.GridSearchCV` to tune a preprocessing pipeline.
+ [Word Embeddings with Word2Vec](https://github.com/chrisfalter/DataScience/blob/master/NLP/word2vec.ipynb) - In the early days of NLP--not long after the invention of papyrus, it seems!--words were represented as vectors of length N, where N is the vocabulary size. This has two disadvantages: (1) Semantic relationships between words are unavailable; and (2) The dimensionality of the data is extremely high. Word2Vec defeats these problems by embedding each vocabulary word in a much smaller vector space where similar words are mathematically closer than dissimilar words (thereby capturing semantic relationships). Click the notebook link above to see how I created a word2vec embedding over the NLTK Inaugural corpus. Here's [the PDF](https://github.com/chrisfalter/DataScience/blob/master/NLP/word2vec.pdf) if you prefer that format, although you'll need to view the annotated T-SNE projection of sample words [here](https://github.com/chrisfalter/DataScience/blob/master/NLP/tsne.png) because the `nbconvert` tool couldn't render the image into PDF.
